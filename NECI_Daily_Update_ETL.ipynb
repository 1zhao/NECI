{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7ad8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd5bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "GHCND_Url = \"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/USW000{}.csv\"\n",
    "WBAN_Codes = [\"14739\", \"23169\", \"94846\"]\n",
    "Zarr_Archive_Path = \"ghcnd_archive.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ff8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(wban_code):\n",
    "    url = GHCND_Url.format(wban_code)\n",
    "    return pd.read_csv(url, low_memory=False)\n",
    "\n",
    "def transform_data(df):\n",
    "    df = df[['DATE', 'PRCP', 'TMAX','TMIN']].copy()\n",
    "    df.columns = ['time', 'precp', 'tmax', 'tmin']\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace = True)\n",
    "    return df\n",
    "\n",
    "def build_ghcnd_archive(wban_codes):\n",
    "    datasets = []\n",
    "    for code in wban_codes:\n",
    "        df = fetch_data(code)\n",
    "        df = transform_data(df)\n",
    "        ds = xr.Dataset.from_dataframe(df)\n",
    "        ds = ds.expand_dims({\"ghcn_id\":[code]})\n",
    "        datasets.append(ds)\n",
    "        \n",
    "    combined_ds = xr.concat(datasets, dim=\"ghcn_id\")\n",
    "    combined_ds.to_zarr(Zarr_Archive_Path, mode='w', consolidated = True)\n",
    "    print(\"Archived!\")\n",
    "    \n",
    "def update_ghcnd_archive(wban_codes):\n",
    "    archive = xr.open_zarr(Zarr_Archive_Path)\n",
    "    datasets = []\n",
    "    for code in wban_codes:\n",
    "        df = fetch_data(code)\n",
    "        df = transform_data(df)\n",
    "        ds = xr.Dataset.from_dataframe(df)\n",
    "        ds = ds.expand_dims({\"ghcn_id\":[code]})\n",
    "        datasets.append(ds)\n",
    "        \n",
    "    combined_ds = xr.concat(datasets, dim=\"ghcn_id\")\n",
    "    combined_ds.to_zarr(Zarr_Archive_Path, mode='a', append_dim = \"time\",consolidated=True)\n",
    "    print(\"Archive Updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a28cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archived!\n"
     ]
    }
   ],
   "source": [
    "# Build the archive from scratch\n",
    "build_ghcnd_archive(WBAN_Codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b559cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive Updated!\n"
     ]
    }
   ],
   "source": [
    "update_ghcnd_archive(WBAN_Codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520c0ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (ghcn_id: 3, time: 64706)\n",
      "Coordinates:\n",
      "  * ghcn_id  (ghcn_id) <U5 '14739' '23169' '94846'\n",
      "  * time     (time) datetime64[ns] 1936-01-01 1936-01-02 ... 2024-07-29\n",
      "Data variables:\n",
      "    precp    (ghcn_id, time) float64 dask.array<chunksize=(2, 16177), meta=np.ndarray>\n",
      "    tmax     (ghcn_id, time) float64 dask.array<chunksize=(2, 16177), meta=np.ndarray>\n",
      "    tmin     (ghcn_id, time) float64 dask.array<chunksize=(2, 16177), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "archive = xr.open_zarr(\"ghcnd_archive.zarr\")\n",
    "print(archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d565f",
   "metadata": {},
   "source": [
    "### How would you orchestrate this system to run at scale?\n",
    "\n",
    "I would use a workflow orchestration tool to schedule daily updates, such as <strong>Databricks Workflow</strong> or any other tools based on the team's preference. The tool would manage the ETL process, schedule tasks, and provide monitoring, also the processed archived files can be shared to colleage effortless. \n",
    "\n",
    "### What major risks would this system face?\n",
    "\n",
    "Data Quaility: If the source data changes structure or Changes in external APIs<strong>(ncei.noaa.gov Down)</strong>  can disrupt the ETL process.It could break the ETL process. For example, missing of some day's data for some of the station.\n",
    "\n",
    "Failed data downloads due to network problems could lead to incomplete or missing data.\n",
    "\n",
    "Handling over 100k stations requires efficient data processing and storage management to avoid performance bottlenecks.\n",
    "\n",
    "\n",
    "\n",
    "### What are the next set of enhancements you would add?\n",
    "\n",
    "Add some checks to make sure the data we're getting is complete, accurate, and consistent. This will help us catch any issues early on.\n",
    "\n",
    "Improve our error handling and logging so we can quickly diagnose and fix any problems. This means better diagnostics and quicker troubleshooting.\n",
    "\n",
    "Fine-tune how we store data in Zarr to handle larger datasets efficiently. We'll tweak the chunking strategy based on how the data will be used next to make sure it's fast and efficient.\n",
    "\n",
    "\n",
    "Chuncking stratgies and the task execution frequency will be based on the next step usage of the data.\n",
    "\n",
    "\n",
    "Update the code to use multithreading and Spark (Databricks) for better scalability. This will help us efficiently handle data from over 100,000 stations.\n",
    "\n",
    "\n",
    "### How would you improve the clarity of this assignment?\n",
    "\n",
    "\n",
    "Specify the required frequency task execution, which will impact the chuncking stratgies pretty much, as well as task execution progress as well as cost of task execution.\n",
    "\n",
    "Clarify whether the script should handle historical data backfilling.\n",
    "\n",
    "Provide this project background information as well as potential usage of the output archieve file, which can help interviewer to optimize the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d610918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d88512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
